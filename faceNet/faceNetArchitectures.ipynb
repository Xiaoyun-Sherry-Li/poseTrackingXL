{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def s1(inputs_shape, data_augmentation, base_filters=25):\n",
    "    nViews = inputs_shape[2]\n",
    "    inputs = keras.Input(inputs_shape)\n",
    "    # normalize inputs jointly\n",
    "    x = preprocessing.Rescaling(scale=1. / 127.5, offset=-1.0)(inputs)\n",
    "    # slice image by view/channel\n",
    "    allSlices = layers.Lambda(lambda z: tf.split(z, nViews, axis=-1))(inputs)\n",
    "\n",
    "    # define view-encoding network\n",
    "    view_in = layers.Input(allSlices[0].shape[1:])\n",
    "    # augment images for each channel independently\n",
    "    if data_augmentation is not None:\n",
    "        x = data_augmentation(view_in)\n",
    "    else:\n",
    "        x = view_in\n",
    "    # big, strided conv and maxpool, 128->32\n",
    "    x = layers.Conv2D(filters=base_filters, kernel_size=7, padding='same', strides=2,\n",
    "                      activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    # Repeated fine-scale conv and maxpool\n",
    "    # 32->16\n",
    "    x = layers.Conv2D(filters=base_filters * 2, kernel_size=3, padding='same',\n",
    "                      activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    # 16->8\n",
    "    x = layers.Conv2D(filters=base_filters * 4, kernel_size=3, padding='same',\n",
    "                      activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    # 8->1\n",
    "    x = layers.Conv2D(filters=base_filters * 8, kernel_size=3, padding='same',\n",
    "                      activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    # Apply dropout on both sides of a hidden layer\n",
    "    x = layers.AlphaDropout(0.25)(x)\n",
    "    view_features = layers.Dense(base_filters*2, name = 'view_features', activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    view_features = layers.AlphaDropout(0.25)(view_features)\n",
    "    # Define predictions and view-fusion weights from features/hidden layer\n",
    "    view_pred = layers.Dense(1, activation='sigmoid', name='view_pred')(view_features)\n",
    "    view_weight = layers.Dense(1, name='view_weight')(view_features)\n",
    "    enc = keras.Model(view_in,[view_pred, view_weight])\n",
    "\n",
    "    # Make predictions on each image and gather results\n",
    "    allPreds = [enc(x) for x in allSlices]\n",
    "    avg_pred = layers.Average(name='avg_pred')([x[0] for x in allPreds])\n",
    "    # Take weighted combination of view preds to make joint prediction\n",
    "    view_preds = layers.Concatenate(name='view_preds')([x[0] for x in allPreds])\n",
    "    view_weights = layers.Concatenate(name='view_weights')([x[1] for x in allPreds])\n",
    "    view_weights = layers.Softmax()(view_weights)\n",
    "    joint_pred = layers.Dot(axes=1, name='joint_pred')([view_preds, view_weights])\n",
    "\n",
    "    # Define full model\n",
    "    return keras.Model(inputs, [joint_pred, avg_pred])\n",
    "\n",
    "def s2(inputs_shape, data_augmentation, base_filters=16):\n",
    "    nViews = inputs_shape[2]\n",
    "    inputs = keras.Input(inputs_shape)\n",
    "    # normalize inputs jointly\n",
    "    x = preprocessing.Rescaling(scale=1. / 127.5, offset=-1.0)(inputs)\n",
    "    # slice image by view/channel\n",
    "    # allSlices = [layers.Lambda(lambda z: z[...,nView:nView+1])(x) for nView in range(nViews)]\n",
    "    allSlices = []\n",
    "    for nView in range(nViews):\n",
    "        allSlices.append(layers.Lambda(lambda z: z[...,nView:nView+1])(x))\n",
    "\n",
    "    # define view-encoding network\n",
    "    view_in = layers.Input(allSlices[0].shape[1:])\n",
    "    # augment images for each channel independently\n",
    "    if data_augmentation is not None:\n",
    "        x = data_augmentation(view_in)\n",
    "    else:\n",
    "        x = view_in\n",
    "    # big, strided conv and maxpool, 128->32\n",
    "    x = layers.Conv2D(filters=base_filters, kernel_size=7, padding='same', strides=2,\n",
    "                      activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    # Repeated fine-scale conv and maxpool\n",
    "    # 32->16\n",
    "    x = layers.Conv2D(filters=base_filters * 2, kernel_size=3, padding='same',\n",
    "                      activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    # 16->8\n",
    "    x = layers.Conv2D(filters=base_filters * 4, kernel_size=3, padding='same',\n",
    "                      activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    # 8->1\n",
    "    x = layers.Conv2D(filters=base_filters * 8, kernel_size=3, padding='same',\n",
    "                      activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    view_features = layers.GlobalAveragePooling2D()(x)\n",
    "    view_pred = layers.Dense(1, name='view_pred')(view_features)\n",
    "    # define encoding model\n",
    "    enc = keras.Model(view_in,view_pred)\n",
    "\n",
    "    # Combine predictions from each view\n",
    "    allPreds = layers.Concatenate()([enc(x) for x in allSlices])\n",
    "    joint_pred = layers.Dense(1, activation='sigmoid', name='joint_pred')(allPreds)\n",
    "\n",
    "    # Define full model\n",
    "    return keras.Model(inputs, joint_pred)\n",
    "\n",
    "def s3(inputs_shape, data_augmentation, base_filters=25):\n",
    "    inputs = keras.Input(inputs_shape)\n",
    "    # normalize inputs\n",
    "    x = preprocessing.Rescaling(scale=1. / 127.5, offset=-1.0)(inputs)\n",
    "    # augment images\n",
    "    if data_augmentation is not None:\n",
    "        x = data_augmentation(x)\n",
    "    # big, strided conv and maxpool, 128->32\n",
    "    x = layers.Conv2D(filters=base_filters, kernel_size=7, padding='same', strides=2,\n",
    "                      activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    # Repeated fine-scale conv and maxpool\n",
    "    # 32->16\n",
    "    x = layers.Conv2D(filters=base_filters * 2, kernel_size=3, padding='same',\n",
    "                      activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    # 16->8\n",
    "    x = layers.Conv2D(filters=base_filters * 4, kernel_size=3, padding='same',\n",
    "                      activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    # 8->4\n",
    "    x = layers.Conv2D(filters=base_filters * 8, kernel_size=3, padding='same',\n",
    "                      activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    # flatten, featurize and dropout, then predict\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.AlphaDropout(0.25)(x)\n",
    "    view_features = layers.Dense(base_filters*2, name = 'view_features', activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    view_features = layers.AlphaDropout(0.25)(view_features)\n",
    "    view_pred = layers.Dense(1, activation='sigmoid', name='view_pred')(view_features)\n",
    "    # return model\n",
    "    return keras.Model(inputs, view_pred)\n",
    "\n",
    "def s4(inputs_shape=(128,128,1), data_augmentation=None, base_filters=25):\n",
    "    inputs = keras.Input(inputs_shape)\n",
    "    # normalize inputs\n",
    "    x = preprocessing.Rescaling(scale=1. / 127.5, offset=-1.0)(inputs)\n",
    "    # augment images\n",
    "    if data_augmentation is not None:\n",
    "        x = data_augmentation(x)\n",
    "    # big, strided conv and maxpool, 128->32\n",
    "    x = layers.Conv2D(filters=base_filters, kernel_size=7, padding='same', strides=2,\n",
    "                      activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    # Repeated fine-scale conv and maxpool\n",
    "    # 32->16\n",
    "    x = layers.Conv2D(filters=base_filters * 2, kernel_size=3, padding='same',\n",
    "                      activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    # 16->8\n",
    "    x = layers.Conv2D(filters=base_filters * 4, kernel_size=3, padding='same',\n",
    "                      activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    # 8->1\n",
    "    x = layers.Conv2D(filters=base_filters * 8, kernel_size=3, padding='same',\n",
    "                      activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    x = layers.GlobalAveragePooling2D(name='view_features')(x)\n",
    "    # dropout, prediction, and return model\n",
    "    x = layers.AlphaDropout(0.2)(x)\n",
    "    view_pred = layers.Dense(1, activation='sigmoid', name='view_pred')(x)\n",
    "    return keras.Model(inputs, view_pred)\n",
    "\n",
    "def s5(inputs_shape=(128,128,1), data_augmentation=None, base_filters=25):\n",
    "    inputs = keras.Input(inputs_shape)\n",
    "    # normalize inputs\n",
    "    x = preprocessing.Rescaling(scale=1. / 127.5, offset=-1.0)(inputs)\n",
    "    # augment images\n",
    "    if data_augmentation is not None:\n",
    "        x = data_augmentation(x)\n",
    "    # big, strided conv and maxpool, 128->32\n",
    "    x = layers.Conv2D(filters=base_filters, kernel_size=7, padding='same', strides=2,\n",
    "                      activation='selu', kernel_initializer='lecun_normal')(x) # base filter: depth, kernel size: look at 7 x 7 region at a time, stride: downsample by 2, selu: non-linearlity\n",
    "    x = layers.Conv2D(filters=base_filters*2, kernel_size=3, padding='same',\n",
    "                      activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    x = layers.MaxPool2D(2)(x) # reduce dimensions, focusing on the most prominent features \n",
    "    # Repeated fine-scale conv and maxpool\n",
    "    # 32->16\n",
    "    x = layers.Conv2D(filters=base_filters * 2, kernel_size=3, padding='same',\n",
    "                      activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    x = layers.Conv2D(filters=base_filters * 4, kernel_size=3, padding='same',\n",
    "                      activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    # 16->1\n",
    "    x = layers.Conv2D(filters=base_filters * 4, kernel_size=3, padding='same',\n",
    "                      activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    x = layers.GlobalAveragePooling2D(name='view_features')(x)\n",
    "    # dropout, prediction, and return model\n",
    "    x = layers.AlphaDropout(0.2)(x)\n",
    "    view_pred = layers.Dense(1, activation='sigmoid', name='view_pred')(x)\n",
    "    return keras.Model(inputs, view_pred)\n"
   ],
   "id": "35adbf38185df599"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def j1(inputs_shape, viewMdl):\n",
    "    nViews = inputs_shape[-1]\n",
    "    inputs = layers.Input(inputs_shape)\n",
    "    allSlices = layers.Lambda(lambda z: tf.split(z, nViews, axis=-1))(inputs)\n",
    "\n",
    "    # find feature and prediction layers in the single-view model\n",
    "    feature_layer_num = [i for i,x in enumerate(viewMdl.layers) if x.name=='view_features'][0]\n",
    "    pred_layer_num = [i for i, x in enumerate(viewMdl.layers) if x.name == 'view_pred'][0]\n",
    "\n",
    "    # set 'trainable' to false for all layers (optional, train final layer?)\n",
    "    for l in viewMdl.layers[:feature_layer_num]:\n",
    "        l.trainable = False\n",
    "\n",
    "    # gather features and predictions to define new model w/ weighting output\n",
    "    # optionally, use a hidden layer here between feature and weighting layer\n",
    "    feature_layer = viewMdl.layers[feature_layer_num]\n",
    "    x = layers.AlphaDropout(0.2, name='feature_dropout')(feature_layer.output)\n",
    "    x = layers.Dense(10, name='feature_hidden', activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    view_weight = layers.Dense(1, name='view_weight')(x)\n",
    "    pred_layer = viewMdl.layers[pred_layer_num]\n",
    "    featMdl = keras.Model(viewMdl.input, [pred_layer.output, view_weight])\n",
    "\n",
    "    # Make predictions and average\n",
    "    allPreds = [featMdl(view) for view in allSlices]\n",
    "    avg_pred = layers.Average(name='avg_pred')([x[0] for x in allPreds])\n",
    "\n",
    "    # Take weighted combination of view preds to make joint prediction\n",
    "    view_preds = layers.Concatenate(name='view_preds')([x[0] for x in allPreds])\n",
    "    view_weights = layers.Concatenate(name='view_weights')([x[1] for x in allPreds])\n",
    "    view_weights = layers.Softmax()(view_weights)\n",
    "    joint_pred = layers.Dot(axes=1, name='joint_pred')([view_preds, view_weights])\n",
    "\n",
    "    return keras.Model(inputs, [joint_pred, avg_pred])\n",
    "\n",
    "def j2(inputs_shape, viewMdl):\n",
    "    nViews = inputs_shape[-1]\n",
    "    inputs = layers.Input(inputs_shape)\n",
    "    allSlices = layers.Lambda(lambda z: tf.split(z, nViews, axis=-1))(inputs)\n",
    "\n",
    "    # find feature and prediction layers in the single-view model\n",
    "    feature_layer_num = [i for i,x in enumerate(viewMdl.layers) if x.name=='view_features'][0]\n",
    "    pred_layer_num = [i for i, x in enumerate(viewMdl.layers) if x.name == 'view_pred'][0]\n",
    "\n",
    "    # lock weights for layers before feature layer\n",
    "    for l in viewMdl.layers[:feature_layer_num]:\n",
    "        l.trainable = False\n",
    "\n",
    "    # gather features and pred from pretrained model, add hidden layer and new weighting outputs\n",
    "    feature_layer = viewMdl.layers[feature_layer_num]\n",
    "    x = layers.AlphaDropout(0.2, name='feature_dropout')(feature_layer.output)\n",
    "    x = layers.Dense(10, name='feature_hidden', activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    view_weight = layers.Dense(1, name='view_weight')(x)\n",
    "    pred_layer = viewMdl.layers[pred_layer_num]\n",
    "    featMdl = keras.Model(viewMdl.input, [pred_layer.output, view_weight])\n",
    "\n",
    "    # Make predictions\n",
    "    allPreds = [featMdl(view) for view in allSlices]\n",
    "    # Take weighted combination of view preds to make joint prediction\n",
    "    view_preds = layers.Concatenate(name='view_preds')([x[0] for x in allPreds])\n",
    "    view_weights = layers.Concatenate(name='view_weights')([x[1] for x in allPreds])\n",
    "    view_weights = layers.Softmax()(view_weights)\n",
    "    joint_pred = layers.Dot(axes=1, name='joint_pred')([view_preds, view_weights])\n",
    "\n",
    "    return keras.Model(inputs, joint_pred)\n",
    "\n",
    "# like j1 but weighted average instead of softmax\n",
    "def j3(inputs_shape, viewMdl):\n",
    "    nViews = inputs_shape[-1]\n",
    "    inputs = layers.Input(inputs_shape)\n",
    "    allSlices = layers.Lambda(lambda z: tf.split(z, nViews, axis=-1))(inputs)\n",
    "\n",
    "    # find feature and prediction layers in the single-view model\n",
    "    feature_layer_num = [i for i,x in enumerate(viewMdl.layers) if x.name=='view_features'][0]\n",
    "    pred_layer_num = [i for i, x in enumerate(viewMdl.layers) if x.name == 'view_pred'][0]\n",
    "\n",
    "    # set 'trainable' to false for all layers (optional, train final layer?)\n",
    "    for l in viewMdl.layers[:feature_layer_num]:\n",
    "        l.trainable = False\n",
    "\n",
    "    # gather features and predictions to define new model w/ weighting output\n",
    "    # optionally, use a hidden layer here between feature and weighting layer\n",
    "    feature_layer = viewMdl.layers[feature_layer_num]\n",
    "    x = layers.AlphaDropout(0.2, name='feature_dropout')(feature_layer.output)\n",
    "    x = layers.Dense(10, name='feature_hidden', activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    view_weight = layers.Dense(1, activation='sigmoid', name='view_weight')(x)\n",
    "    pred_layer = viewMdl.layers[pred_layer_num]\n",
    "    featMdl = keras.Model(viewMdl.input, [pred_layer.output, view_weight])\n",
    "\n",
    "    # Make predictions and average\n",
    "    allPreds = [featMdl(view) for view in allSlices]\n",
    "    avg_pred = layers.Average(name='avg_pred')([x[0] for x in allPreds])\n",
    "\n",
    "    # Take weighted combination of view preds to make joint prediction\n",
    "    view_preds = layers.Concatenate(name='view_preds')([x[0] for x in allPreds])\n",
    "    view_weights = layers.Concatenate(name='view_weights')([x[1] for x in allPreds])\n",
    "    view_weights = layers.Lambda(lambda z: tf.math.divide(z, tf.math.reduce_sum(z, axis=-1, keepdims=True)),\n",
    "                                 name='norm_view_weights')(view_weights)\n",
    "    joint_pred = layers.Dot(axes=1, name='joint_pred')([view_preds, view_weights])\n",
    "\n",
    "    return keras.Model(inputs, [joint_pred, avg_pred])\n",
    "\n",
    "# like j1 but jointPred directly from weights\n",
    "def j4(inputs_shape=(128,128,6), viewMdl=s5()):\n",
    "    nViews = inputs_shape[-1]\n",
    "    inputs = layers.Input(inputs_shape)\n",
    "    allSlices = layers.Lambda(lambda z: tf.split(z, nViews, axis=-1))(inputs)\n",
    "\n",
    "    # find feature and prediction layers in the single-view model\n",
    "    feature_layer_num = [i for i,x in enumerate(viewMdl.layers) if x.name=='view_features'][0]\n",
    "    pred_layer_num = [i for i, x in enumerate(viewMdl.layers) if x.name == 'view_pred'][0]\n",
    "\n",
    "    # set 'trainable' to false for all layers before image features (optional?)\n",
    "    # for l in viewMdl.layers[:feature_layer_num-1]:\n",
    "    #     l.trainable = False\n",
    "\n",
    "    # gather features and predictions to define new model w/ weighting output\n",
    "    # optionally, use a hidden layer here between feature and weighting layer\n",
    "    feature_layer = viewMdl.layers[feature_layer_num]\n",
    "    x = layers.AlphaDropout(0.2, name='feature_dropout')(feature_layer.output)\n",
    "    x = layers.Dense(10, name='feature_hidden', activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    view_weight = layers.Dense(1, name='view_weight')(x)\n",
    "    pred_layer = viewMdl.layers[pred_layer_num]\n",
    "    featMdl = keras.Model(viewMdl.input, [pred_layer.output, view_weight])\n",
    "\n",
    "    # Make predictions and average\n",
    "    allPreds = [featMdl(view) for view in allSlices]\n",
    "    avg_pred = layers.Average(name='avg_pred')([x[0] for x in allPreds])\n",
    "\n",
    "    # Take 'weights'/scores and average, using preset and non-tunable weights\n",
    "    view_weights = layers.Concatenate(name='view_weights')([x[1] for x in allPreds])\n",
    "    joint_pred = layers.Dense(1, activation='sigmoid', name='joint_pred')\n",
    "    joint_pred_out = joint_pred(view_weights)\n",
    "    joint_pred.set_weights([np.ones((6,1)), np.zeros((1))])\n",
    "    joint_pred.trainable = False\n",
    "\n",
    "    return keras.Model(inputs, [joint_pred_out, avg_pred])\n"
   ],
   "id": "51d8ab90bf9e20c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def v1(inputs_shape, data_augmentation):\n",
    "    inputs = keras.Input(inputs_shape)\n",
    "    # normalize inputs\n",
    "    x = preprocessing.Rescaling(scale=1. / 127.5, offset=-1.0)(inputs)\n",
    "    if data_augmentation is not None:\n",
    "        # augment images\n",
    "        x = data_augmentation(x)\n",
    "\n",
    "    # Process each channel independently: Depthwise Convolution, MaxPool2D\n",
    "    # if using filter size 7, stride 2, 32x32 output\n",
    "    x = layers.DepthwiseConv2D(7, strides=2, padding='same', depth_multiplier=10,\n",
    "                               activation='selu', depthwise_initializer='lecun_normal')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    # repeat depthwise conv using small filters and stride 1\n",
    "    x = layers.DepthwiseConv2D(3, strides=1, padding='same', depth_multiplier=2,\n",
    "                               activation='selu', depthwise_initializer='lecun_normal')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    # Compress channels using 1x1 convolution\n",
    "    x = layers.Conv2D(filters=50, kernel_size=1, padding='same', activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "\n",
    "    # Combine across channels: Conv2D, MaxPool2D\n",
    "    # use filter size 3, stride 1, and repeat twice times to go from 16x16 to 4x4\n",
    "    x = layers.Conv2D(filters=25, kernel_size=3, padding='same', activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    x = layers.Conv2D(filters=25, kernel_size=3, padding='same', activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "\n",
    "    # Fully connected layer to combine, then classification head\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(25, activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # define resulting model\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "def v2(inputs_shape, data_augmentation):\n",
    "    inputs = keras.Input(inputs_shape)\n",
    "    # normalize inputs\n",
    "    x = preprocessing.Rescaling(scale=1. / 127.5, offset=-1.0)(inputs)\n",
    "    if data_augmentation is not None:\n",
    "        # augment images\n",
    "        x = data_augmentation(x)\n",
    "\n",
    "    # Process each channel independently and downsample with strided Depthwise Convolutions\n",
    "    # 128->64\n",
    "    x = layers.DepthwiseConv2D(5, strides=2, padding='same', depth_multiplier=10)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    # 64->32\n",
    "    x = layers.DepthwiseConv2D(5, strides=2, padding='same', depth_multiplier=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    # 32->16\n",
    "    x = layers.DepthwiseConv2D(5, strides=2, padding='same', depth_multiplier=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # Compress channels using 1x1 convolution\n",
    "    x = layers.Conv2D(filters=50, kernel_size=1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # Combine across channels and downsample with Conv2D\n",
    "    #16->8\n",
    "    x = layers.Conv2D(filters=25, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    # 8->4\n",
    "    x = layers.Conv2D(filters=25, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # Global Average Pooling to reduce spatial dimension\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    # Classification head directly from global average\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    # define resulting model\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "def v3(inputs_shape, data_augmentation):\n",
    "    inputs = keras.Input(inputs_shape)\n",
    "    # normalize inputs\n",
    "    x = preprocessing.Rescaling(scale=1. / 127.5, offset=-1.0)(inputs)\n",
    "    if data_augmentation is not None:\n",
    "        # augment images\n",
    "        x = data_augmentation(x)\n",
    "\n",
    "    # Process each channel independently and downsample\n",
    "    # 128->32\n",
    "    x = layers.DepthwiseConv2D(7, strides=2, padding='same', depth_multiplier=10)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "\n",
    "    # combine across channels\n",
    "    x = layers.Conv2D(filters=100, kernel_size=1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # Combine across channels and downsample with Conv2D\n",
    "    #32->16\n",
    "    x = layers.Conv2D(filters=25, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    # 16->8\n",
    "    x = layers.Conv2D(filters=25, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    # 8->4\n",
    "    x = layers.Conv2D(filters=25, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # Global Average Pooling to reduce spatial dimension\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    # Classification head directly from global average\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    # define resulting model\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "def v4(inputs_shape, data_augmentation):\n",
    "    inputs = keras.Input(inputs_shape)\n",
    "    # normalize inputs\n",
    "    x = preprocessing.Rescaling(scale=1. / 127.5, offset=-1.0)(inputs)\n",
    "    if data_augmentation is not None:\n",
    "        # augment images\n",
    "        x = data_augmentation(x)\n",
    "\n",
    "    # Process each channel independently and downsample with strided Depthwise Convolutions\n",
    "    # 128->64\n",
    "    x = layers.DepthwiseConv2D(5, strides=2, padding='same', depth_multiplier=10)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    # 64->32\n",
    "    x = layers.DepthwiseConv2D(5, strides=2, padding='same', depth_multiplier=5)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    # 32->16\n",
    "    x = layers.DepthwiseConv2D(5, strides=2, padding='same', depth_multiplier=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # Compress channels using 1x1 convolution\n",
    "    x = layers.Conv2D(filters=100, kernel_size=1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # Combine across channels and downsample with Conv2D\n",
    "    #16->8\n",
    "    x = layers.Conv2D(filters=40, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    # 8->4\n",
    "    x = layers.Conv2D(filters=40, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # Global Average Pooling to reduce spatial dimension\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    # Classification head directly from global average\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    # define resulting model\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "def v5(inputs_shape, data_augmentation):\n",
    "    inputs = keras.Input(inputs_shape)\n",
    "    # normalize inputs\n",
    "    x = preprocessing.Rescaling(scale=1. / 127.5, offset=-1.0)(inputs)\n",
    "    if data_augmentation is not None:\n",
    "        # augment images\n",
    "        x = data_augmentation(x)\n",
    "\n",
    "    # Process each channel independently and downsample with strided Depthwise Convolutions\n",
    "    # 128->64\n",
    "    x = layers.DepthwiseConv2D(5, strides=2, padding='same', depth_multiplier=20)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    # 64->32\n",
    "    x = layers.DepthwiseConv2D(5, strides=2, padding='same', depth_multiplier=10)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    # 32->16\n",
    "    x = layers.DepthwiseConv2D(5, strides=2, padding='same', depth_multiplier=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # Compress channels using 1x1 convolution\n",
    "    x = layers.Conv2D(filters=120, kernel_size=1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # Combine across channels and downsample with Conv2D\n",
    "    #16->8\n",
    "    x = layers.Conv2D(filters=30, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    # 8->4\n",
    "    x = layers.Conv2D(filters=30, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # Global Average Pooling to reduce spatial dimension\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    # Classification head directly from global average\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    # define resulting model\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "def v6(inputs_shape, data_augmentation):\n",
    "    inputs = keras.Input(inputs_shape)\n",
    "    # normalize inputs\n",
    "    x = preprocessing.Rescaling(scale=1. / 127.5, offset=-1.0)(inputs)\n",
    "    if data_augmentation is not None:\n",
    "        # augment images\n",
    "        x = data_augmentation(x)\n",
    "\n",
    "    # Process each channel independently and downsample with strided Depthwise Convolutions\n",
    "    # 128->64\n",
    "    x = layers.DepthwiseConv2D(5, strides=2, padding='same', depth_multiplier=10)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    # 64->32\n",
    "    x = layers.DepthwiseConv2D(5, strides=2, padding='same', depth_multiplier=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    # 32->16\n",
    "    x = layers.DepthwiseConv2D(5, strides=2, padding='same', depth_multiplier=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # Compress channels using 1x1 convolution\n",
    "    x = layers.Conv2D(filters=120, kernel_size=1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # Combine across channels and downsample with Conv2D\n",
    "    #16->8\n",
    "    x = layers.Conv2D(filters=60, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    # 8->4\n",
    "    x = layers.Conv2D(filters=60, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # Global Average Pooling to reduce spatial dimension\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    # Classification head directly from global average\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    # define resulting model\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "def v7(inputs_shape, data_augmentation):\n",
    "    inputs = keras.Input(inputs_shape)\n",
    "    # normalize inputs\n",
    "    x = preprocessing.Rescaling(scale=1. / 127.5, offset=-1.0)(inputs)\n",
    "    if data_augmentation is not None:\n",
    "        # augment images\n",
    "        x = data_augmentation(x)\n",
    "\n",
    "    # Process each channel independently and downsample with strided Depthwise Convolutions\n",
    "    # 128->64\n",
    "    x = layers.DepthwiseConv2D(5, strides=2, padding='same', depth_multiplier=10)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    # 64->32\n",
    "    x = layers.DepthwiseConv2D(5, strides=2, padding='same', depth_multiplier=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    # 32->16\n",
    "    x = layers.DepthwiseConv2D(5, strides=2, padding='same', depth_multiplier=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    # Compress channels using 1x1 convolution\n",
    "    x = layers.Conv2D(filters=100, kernel_size=1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    # Combine across channels and downsample with Conv2D\n",
    "    #16->8\n",
    "    x = layers.Conv2D(filters=50, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    # 8->4\n",
    "    x = layers.Conv2D(filters=50, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # Global Average Pooling to reduce spatial dimension\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    # Classification head directly from global average\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    # define resulting model\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "def v8(inputs_shape, data_augmentation):\n",
    "    inputs = keras.Input(inputs_shape)\n",
    "    # normalize inputs\n",
    "    x = preprocessing.Rescaling(scale=1. / 127.5, offset=-1.0)(inputs)\n",
    "    if data_augmentation is not None:\n",
    "        # augment images\n",
    "        x = data_augmentation(x)\n",
    "\n",
    "    # Process each channel independently: Depthwise Convolution, MaxPool2D\n",
    "    # if using filter size 7, stride 2, 128->32 output\n",
    "    x = layers.DepthwiseConv2D(7, strides=2, padding='same', depth_multiplier=10,\n",
    "                               activation='selu', depthwise_initializer='lecun_normal')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    # Combine across channels: Conv2D, MaxPool2D\n",
    "    # use filter size 3, stride 1 (32->16)\n",
    "    x = layers.Conv2D(filters=100, kernel_size=3, padding='same', activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    # 16->8\n",
    "    x = layers.Conv2D(filters=100, kernel_size=3, padding='same', activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    # 8->4\n",
    "    x = layers.Conv2D(filters=100, kernel_size=3, padding='same', activation='selu', kernel_initializer='lecun_normal')(\n",
    "        x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    # Fully connected layer to combine, then classification head\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(25, activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # define resulting model\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "def v9(inputs_shape, data_augmentation):\n",
    "    inputs = keras.Input(inputs_shape)\n",
    "    # normalize inputs\n",
    "    x = preprocessing.Rescaling(scale=1. / 127.5, offset=-1.0)(inputs)\n",
    "    if data_augmentation is not None:\n",
    "        # augment images\n",
    "        x = data_augmentation(x)\n",
    "\n",
    "    # Process each channel independently: Depthwise Convolution, MaxPool2D\n",
    "    # if using filter size 7, stride 2, 128->32 output\n",
    "    x = layers.DepthwiseConv2D(7, strides=2, padding='same', depth_multiplier=10,\n",
    "                               activation='selu', depthwise_initializer='lecun_normal')(x)\n",
    "    x = layers.Conv2D(filters=60, kernel_size=1, padding='same', activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    y = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    # use filter size 3, stride 1 (32->16)\n",
    "    x = layers.DepthwiseConv2D(3, padding='same', depth_multiplier=2,\n",
    "                               activation='selu', depthwise_initializer='lecun_normal')(x)\n",
    "    x = layers.Conv2D(filters=100, kernel_size=1, padding='same', activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    # 16->8\n",
    "    x = layers.DepthwiseConv2D(3, padding='same', depth_multiplier=2,\n",
    "                               activation='selu', depthwise_initializer='lecun_normal')(x)\n",
    "    x = layers.Conv2D(filters=100, kernel_size=1, padding='same', activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    # 8->4\n",
    "    x = layers.DepthwiseConv2D(3, padding='same', depth_multiplier=2,\n",
    "                               activation='selu', depthwise_initializer='lecun_normal')(x)\n",
    "    x = layers.Conv2D(filters=100, kernel_size=1, padding='same', activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    x = layers.MaxPool2D(2)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    # Fully connected layer to combine\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(25, activation='selu', kernel_initializer='lecun_normal')(x)\n",
    "    # Same for intermediate supervision output\n",
    "    y = layers.Dense(25, activation='selu', kernel_initializer='lecun_normal')(y)\n",
    "    # Combine both for output\n",
    "    outputs = [layers.Dense(1, activation='sigmoid', name='model_output')(x),\n",
    "               layers.Dense(1, activation='sigmoid', name='int_output')(y)]\n",
    "\n",
    "    # define resulting model\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n"
   ],
   "id": "681da88a41a850c5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
