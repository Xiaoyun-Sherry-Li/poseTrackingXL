{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:40:36.975633Z",
     "start_time": "2025-01-12T04:40:36.943988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "# from tensorflow.keras.models import load_model as tf_load\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"C:/Users/xl313/OneDrive/Documents/GitHub/poseTrackingXL/utils\")\n",
    "from load_matlab_data import loadmat_sbx\n",
    "from slp_utils import posture_tracker\n",
    "import scipy.io\n",
    "from slp_utils import create_slp_project, crop_from_com"
   ],
   "id": "1ede65d5",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "bf60c241",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:40:37.023049Z",
     "start_time": "2025-01-12T04:40:36.975633Z"
    }
   },
   "source": [
    "''' UPDATE data params as appropriate'''\n",
    "# cam params\n",
    "cam_ids = ['blue_cam', 'green_cam', 'red_cam', 'yellow_cam'] # check the input order\n",
    "im_w = 2200\n",
    "im_h = 650\n",
    "# video params\n",
    "start_frame = 35500 # in frames at 50fps # (XL, 010825: an exampler coconut caching/eating + drinking water snippet: 11:50 - 13:00 min in SLV123_110824_wEphys) \n",
    "nFrames = 2500 # in frames at 50fps # take 50sec  # used to be 2500 frames, start at 35500\n",
    "\n",
    "''' UPDATE paths as needed '''\n",
    "# videos\n",
    "root_dir = \"Z:/Sherry/poseTrackingXL/training_files/raw_acquisition_copy/\"\n",
    "vid_root = f\"{root_dir}SLV123_110824_wEphys/\"\n",
    "# camera params\n",
    "cam_params = loadmat_sbx(\"Z:/Sherry/poseTrackingXL/calibration_files/all_opt_arrays/102324_negated_camParams\")['camParams_negateR'] #['camParams']\n",
    "\n",
    "# to save\n",
    "pred_date = \"011125\"\n",
    "# save_file = f'{pred_date}_posture_2stage.npy' # python\n",
    "save_file = f'{pred_date}_posture_2stage.mat' # matlab\n",
    "save_path = f\"{vid_root}{save_file}\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z:/Sherry/poseTrackingXL/calibration_files/all_opt_arrays/102324_negated_camParams\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "a2e31d02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:40:37.070243Z",
     "start_time": "2025-01-12T04:40:37.054234Z"
    }
   },
   "source": [
    "# models\n",
    "comNet = \"Z:/Sherry/poseTrackingXL/training_files/SLP/models/010725_com250107_235615.single_instance.n=460\" \n",
    "postureNet = \"Z:/Sherry/poseTrackingXL/training_files/SLP/models/010825_postureNet250108_164045.single_instance.n=460\"\n",
    "# faceNet = \"Z:\\Selmaan\\DPK-transfer\\j4-v4\"\n",
    "# if running face model, otherwise set to None\n",
    "# face_model = tf_load(faceNet, custom_objects={'tf': tf})\n",
    "face_model = None"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:40:37.101978Z",
     "start_time": "2025-01-12T04:40:37.085966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' set up for this run '''\n",
    "# set up GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ],
   "id": "e433f3c0",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:40:38.975725Z",
     "start_time": "2025-01-12T04:40:37.118014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define the video reader for each camera\n",
    "all_readers = []\n",
    "for i in range(len(cam_ids)):\n",
    "    cam = cam_ids[i]\n",
    "    print(cam)\n",
    "    camPath = f\"{vid_root}{cam}.avi\"\n",
    "\n",
    "    # define the video reader obj and settings\n",
    "    api_id = cv2.CAP_FFMPEG\n",
    "    reader = cv2.VideoCapture(camPath, api_id)\n",
    "    if start_frame > 0:\n",
    "        reader.set(cv2.CAP_PROP_FRAME_COUNT, start_frame)\n",
    "    all_readers.append(reader)"
   ],
   "id": "6ec1fbff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_cam\n",
      "green_cam\n",
      "red_cam\n",
      "yellow_cam\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "87692c79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:41:02.727669Z",
     "start_time": "2025-01-12T04:40:39.007420Z"
    }
   },
   "source": [
    "''' track posture '''\n",
    "obj = posture_tracker(all_readers, cam_params,\n",
    "                        com_model=comNet,\n",
    "                        posture_model=postureNet,\n",
    "                        face_model=face_model)\n",
    "results = obj.track_video(start_frame=start_frame,\n",
    "                            nFrames=nFrames)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Camera Parameters\n",
      "cameraMats:  [array([[ 2.41702718e+03, -1.58864212e+02,  6.21403613e-01],\n",
      "       [-7.25358959e+02, -1.94132218e+02,  7.01161060e-01],\n",
      "       [-3.85120771e+02, -2.35069008e+03, -3.49615100e-01],\n",
      "       [ 8.78300401e+05,  1.94139233e+05,  8.01105665e+02]]), array([[-2.30149020e+03,  1.39717560e+02, -7.21147459e-01],\n",
      "       [ 1.05825604e+03,  1.78721097e+02, -6.07948470e-01],\n",
      "       [-3.30093947e+02, -2.35235430e+03, -3.32182180e-01],\n",
      "       [ 9.22424175e+05,  2.26582211e+05,  8.32531629e+02]]), array([[-7.72851025e+02, -1.00895733e+02,  7.05399101e-01],\n",
      "       [-2.44458657e+03,  9.66003973e+01, -6.37337998e-01],\n",
      "       [-3.51860470e+02, -2.40742776e+03, -3.10181213e-01],\n",
      "       [ 8.78909726e+05,  2.54698363e+05,  8.31167280e+02]]), array([[ 1.06196576e+03,  1.60508099e+02, -6.11385346e-01],\n",
      "       [ 2.31181050e+03, -2.02864429e+02,  7.18479141e-01],\n",
      "       [-3.86794424e+02, -2.35450657e+03, -3.31655970e-01],\n",
      "       [ 9.24258204e+05,  2.09566786e+05,  8.23582079e+02]])]\n",
      "Reading and Predicting\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_2561536\\3413958182.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m                         face_model=face_model)\n\u001B[0;32m      6\u001B[0m results = obj.track_video(start_frame=start_frame,\n\u001B[1;32m----> 7\u001B[1;33m                             nFrames=nFrames)\n\u001B[0m",
      "\u001B[1;32mC:/Users/xl313/OneDrive/Documents/GitHub/poseTrackingXL/utils\\slp_utils.py\u001B[0m in \u001B[0;36mtrack_video\u001B[1;34m(self, start_frame, nFrames)\u001B[0m\n\u001B[0;32m    427\u001B[0m             \u001B[1;32mfor\u001B[0m \u001B[0mnCam\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnCams\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    428\u001B[0m                 \u001B[0mflag\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mimg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreaders\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mnCam\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 429\u001B[1;33m                 \u001B[0mimg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcv2\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcvtColor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcv2\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mCOLOR_BGR2RGB\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# SHERRY added this bcs cv2 image reader reads in BGR, need to convert to RGB to read in the images correctly\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    430\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    431\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mnFrame\u001B[0m \u001B[1;33m<\u001B[0m \u001B[0mstart_frame\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "a30042ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T04:41:02.790950900Z",
     "start_time": "2025-01-12T03:54:09.123103Z"
    }
   },
   "source": [
    "# for matlab \n",
    "scipy.io.savemat(save_path,{\"posture_preds\": results['posture_preds'], \"posture_reproj\": results['posture_rep_err'],\n",
    "                     \"posture_rawpreds\": results['posture_rawpred'], \"com_preds\": results['com_preds'], \"com_reproj\": results['com_rep_err'],\n",
    "                     \"posture_conf\":results['posture_conf'], \"com_conf\":results['com_conf'], #  \"face_preds\":results['face_preds'], \"startTime\": startTime,\n",
    "                     \"camNames\": cam_ids, \"session\": vid_root, \"nFrames\": nFrames,\n",
    "                     \"camParams\": cam_params, }) # \"rawPostures\":sleap_raw_predicted_points_scale_back"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # to save the ds_img (cropped from comNet) to put it in Lightning Pose\n",
    "# LP_cropped_images = results[\"cropped_unseen_images\"]\n",
    "# np.shape(LP_cropped_images)\n",
    "# from PIL import Image\n",
    "# import os\n",
    "# # Directory to save the temporary images\n",
    "# output_dir = \"Z:/Sherry/poseTrackingXL/training_files/posture_vids/new_images\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# \n",
    "# # Convert each .npy to an image and save it\n",
    "# for i, img_array in enumerate(LP_cropped_images):\n",
    "#     img = Image.fromarray(img_array.astype(np.uint8))  # Convert to uint8 format for images\n",
    "#     img.save(os.path.join(output_dir, f\"image_{i+1}.png\")) "
   ],
   "id": "9623cb0bac9382af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# sys.path.append(\"C:/Users/xl313/OneDrive/Documents/GitHub/poseTrackingXL/utils\")\n",
    "# from slp_utils import create_slp_project, crop_from_com\n",
    "# slp_project_dir = 'Z:/Sherry/poseTrackingXL/training_files/' # this is saved locally (at least temporaily instead of on locker)\n",
    "# slp_project_file = f'011125_combined_net.slp'\n",
    "# slp_project_path = f'{slp_project_dir}{slp_project_file}'\n",
    "# skeleton_file = 'C:/Users/xl313/OneDrive/Documents/GitHub/poseTrackingXL/postureNet/posture_skeleton_IL.csv'\n",
    "# \n",
    "# training_vid_dir = 'Z:/Sherry/poseTrackingXL/training_files/'\n",
    "# vid_file = f'011125_combined_pred_vid.npy'\n",
    "# training_vid_path = f'{training_vid_dir}{vid_file}'\n",
    "# np.save(training_vid_path, results['unseen_images'])\n",
    "# # optional, create a sleap object to visualise inference results in sleap GUI.\n",
    "# create_slp_project(vid_path=training_vid_path, \n",
    "#                    skeleton_file=skeleton_file,\n",
    "#                    keypoints=results['sleap_raw_predicted_points_scale_back'],\n",
    "#                    slp_labels_file=slp_project_path)"
   ],
   "id": "fef7303e09d53535",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
