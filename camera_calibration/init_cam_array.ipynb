{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera Calibration Initialization\n",
    "For 3D pose tracking, we need to know how the image from each camera relates to the scene playing out in the world. To understand this relationship, we need to estimate an array of camera parameters, which will then be optimized using the `pySBA.py` bundle adjustment algorithm. These are just initialization parameters, so they don't need to be perfect. Instead, we will aim for estimating them in a way basically reflects reality while keeping the math easy.\n",
    "\n",
    "\n",
    "The first set of parameters are the camera **extrinsics**. These define where each camera is relative to the world (the arena). They can be broken down into a *rotation* (how the world coordinate axes relate to the camera coordinate axes) and a *translation* (how the world origin relates to the camera origin). We will estimate these empirically below.\n",
    "\n",
    "\n",
    "The second set of parameters are the camera **intrinsics**. These define how the 3D scene is transformed into a 2D image on the camera sensor. These include the focal length (distance between the lens and the camera sensor), the x and y principle point offsets (how the image center relates to the center of the lens), and the axis skew (a measurement of shear distortion). These are all measured in pixels. We'll calculate these parameters below using what we know about the physical lens, camera sensor, and image cropping.\n",
    "\n",
    "\n",
    "**Resources:**\n",
    "- Selmaan's [evernote document](https://www.evernote.com/shard/s552/sh/ec4d276d-14de-43e4-b5c9-61f2404d9951/8d825d89cb5b9b1cc9099363bfa6d8da![image.png](attachment:image.png) on how he calibrated his cameras.\n",
    "- 3-part [blog post](http://ksimek.github.io/2012/08/14/decompose/) about camera extrinsics and intrinsics.\n",
    "- [Laser calibration](https://github.com/JohnsonLabJanelia/laserCalib) github repo from Rob Johnson + Jinyao (Jane) Yan at Janelia (largely copied/modified in my `bird_pose_tracking\\camera_calibration`).\n",
    "- [Python bundle adjustment](https://github.com/jahdiel/pySBA/blob/master/PySBA.py) github repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirically estimate the camera extrinsincs\n",
    "\n",
    "These are needed to initialize the camera array for laser calibration and will be subsequently optimized during the calibration process. The extrinsics roughly define the location of the camera in the world and what direction its pointing in, using a rotation matrix and translation vector.\n",
    "\n",
    "For both world and camera coordinates, we will use a normalized coordinate system where half the sidelength of the arena = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the world coordinates\n",
    "- origin is at the arena center\n",
    "- $z$ is up from the floor\n",
    "- $x$ increases towards the red and blue feeders\n",
    "- $y$ increases towards the yellow and red feeders\n",
    "\n",
    "So the red feeder is at $(x = 1, y = 1, z = 0)$.\n",
    "\n",
    "### Define the camera coordinates\n",
    "- origin is the camera center\n",
    "- $x$ increases to the right of the camera\n",
    "- $y$ increases down from the camera (**note** that world $z$ becomes camera $y$)\n",
    "- $z$ increases away from the camera\n",
    "\n",
    "We can approximate the cameras as being a bit above each of the 4 corners for easy math. In other words, let $h = 1/4$ be the height of the cameras above the arena and let $d = \\sqrt{2}$ be the distance of each camera from the origin.\n",
    "\n",
    "\n",
    "Then the translation vector needed to describe the world origin in terms of camera coordinates is simply $t = [0, h, d]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 1/4\n",
    "d = np.sqrt(2)\n",
    "t = np.asarray([0, h, d])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose points\n",
    "To find the rotation matrix, we need to choose a bunch of points where the world and camera coords are easy to estimate. We'll use the 4 corners, plus a point camera height above the origin. In order of red, yellow, green, blue, above-center.\n",
    "\n",
    "Let $W$ be the world coords of the points $(x, y, z)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points in world coordinates:\n",
      "\n",
      "[[ 1.    1.    0.  ]\n",
      " [-1.    1.    0.  ]\n",
      " [-1.   -1.    0.  ]\n",
      " [ 1.   -1.    0.  ]\n",
      " [ 0.    0.    0.25]]\n"
     ]
    }
   ],
   "source": [
    "world_pts = np.asarray([[1, 1, 0], [-1, 1, 0], [-1, -1, 0], [1, -1, 0], [0, 0, h]])\n",
    "print('points in world coordinates:\\n')\n",
    "print(world_pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also easy to determine the camera coordinates of each of our chosen points for each camera:\n",
    "- The point in the camera's own corner will be at the origin in $x$ and $z$, and $h$ below in y.\n",
    "- The 2 corners adjacent to the camera are $d$ away in $x$ and $z$, and $h$ below.\n",
    "    - Using the right-hand rule, the corner to the right is $d$ in $x$, while the corner to the left is $-d$.\n",
    "- The furthest corner is $2d$ away in $z$, $0$ in $x$, and $h$ below.\n",
    "- The above-center point is $d$ away in $z$, $0$ in $x$ and $y$.\n",
    "\n",
    "Let $C$ be the camera coords of the points $(x, y, z)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points in red camera coordinates:\n",
      "\n",
      "[[ 0.      0.25    0.    ]\n",
      " [ 1.4142  0.25    1.4142]\n",
      " [ 0.      0.25    2.8284]\n",
      " [-1.4142  0.25    1.4142]\n",
      " [ 0.      0.      1.4142]]\n"
     ]
    }
   ],
   "source": [
    "red_pts = np.asarray([[0, h, 0], [d, h, d], [0, h, 2*d], [-d, h, d], [0, 0, d]])\n",
    "yellow_pts = np.asarray([[-d, h, d], [0, h, 0], [d, h, d], [0, h, 2*d], [0, 0, d]])\n",
    "green_pts = np.asarray([[0, h, 2*d], [-d, h, d], [0, h, 0], [d, h, d], [0, 0, d]])\n",
    "blue_pts = np.asarray([[d, h, d], [0, h, 2*d], [-d, h, d], [0, h, 0], [0, 0, d]])\n",
    "\n",
    "cam_pts = (red_pts, yellow_pts, green_pts, blue_pts)\n",
    "\n",
    "print('points in red camera coordinates:\\n')\n",
    "print(np.round(red_pts, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the rotation matrix\n",
    "We can now use least squares to solve the system of equations and find the rotation matrix $R$ needed to rotate the world points $W$ onto their postitions $C$ in the camera frame of reference (accounting for the translation $t$ of each camera):\n",
    "\n",
    "$$WR = C - t$$\n",
    "\n",
    "Then, we'll convert this into a rotation vector for each camera (which is the format needed for the calibration algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ilow1\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "n_cams = len(cam_pts)\n",
    "n_pts = world_pts.shape[0]\n",
    "\n",
    "rotation_mats = []\n",
    "rotation_vecs = np.zeros((n_cams, 3))\n",
    "for i, c in enumerate(cam_pts):\n",
    "    [rot, tmp, tmp, tmp] = np.linalg.lstsq(a=world_pts, b=(c-t))\n",
    "    rotation_mats.append(rot)\n",
    "    r = Rotation.from_matrix(rot)\n",
    "    rotation_vecs[i] = r.as_rotvec()\n",
    "    \n",
    "# for some reason, scipy's rotation formula gives a rotation vector that is sign-flipped...\n",
    "rotation_vecs = -rotation_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the translation vector for all cams is: [0.    0.25  1.414]\n",
      "\n",
      "red cam rotation vector is: [ 0.729  1.76  -1.76 ]\n",
      "yellow cam rotation vector is: [ 0.729 -1.76   1.76 ]\n",
      "green cam rotation vector is: [ 1.482 -0.614  0.614]\n",
      "blue cam rotation vector is: [ 1.482  0.614 -0.614]\n"
     ]
    }
   ],
   "source": [
    "''' Display the translation vector and the rotation vector for each camera '''\n",
    "print(f'the translation vector for all cams is: {np.round(t, 3)}\\n')\n",
    "\n",
    "cams = ['red', 'yellow', 'green', 'blue']\n",
    "for c, r in zip(cams, rotation_vecs):\n",
    "    print(f'{c} cam rotation vector is: {np.round(r, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check our work\n",
    "We can double check that the rotation vectors and matrices are correct by using them to rotate the points from world coordinates to each camera's coordinate frame. To rotate with the rotation vector, we need to use [Rodrigues' rotation formula](https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula). If all goes well, the three outputs below should be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rodrigues_rot(v, k_rot):\n",
    "    '''\n",
    "    Rotate a given vector (v) by a rotation vector (k_rot) using Rodrigues' rotation formula.\n",
    "    '''\n",
    "    theta = np.linalg.norm(k_rot)\n",
    "    k = k_rot / theta\n",
    "    dot = v.dot(k)\n",
    "    cos_theta = np.cos(theta)\n",
    "    sin_theta = np.sin(theta)\n",
    "    \n",
    "    return cos_theta * v + sin_theta * np.cross(k, v) + dot * (1 - cos_theta) * k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.414  0.25   1.414]\n",
      " [-0.     0.25   0.   ]\n",
      " [ 1.414  0.25   1.414]\n",
      " [ 0.     0.25   2.828]\n",
      " [-0.     0.     1.414]]\n"
     ]
    }
   ],
   "source": [
    "''' check that the rotation vector works '''\n",
    "rot_vec = rotation_vecs[cam_idx]\n",
    "rot_vec_pts = np.zeros_like(red_pts)\n",
    "for i, pts in enumerate(world_pts):\n",
    "    rot_vec_pts[i] = rodrigues_rot(pts, rot_vec)\n",
    "rot_vec_pts = rot_vec_pts + t\n",
    "\n",
    "print(np.round(rot_vec_pts, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.414  0.25   1.414]\n",
      " [-0.     0.25   0.   ]\n",
      " [ 1.414  0.25   1.414]\n",
      " [ 0.     0.25   2.828]\n",
      " [ 0.     0.     1.414]]\n"
     ]
    }
   ],
   "source": [
    "''' check that the rotation matrix works '''\n",
    "rot_mat_pts = world_pts.dot(rotation_mats[cam_idx]) + t\n",
    "print(np.round(rot_mat_pts, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.414  0.25   1.414]\n",
      " [ 0.     0.25   0.   ]\n",
      " [ 1.414  0.25   1.414]\n",
      " [ 0.     0.25   2.828]\n",
      " [ 0.     0.     1.414]]\n"
     ]
    }
   ],
   "source": [
    "print(np.round(cam_pts[cam_idx], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the camera intrinsics\n",
    "These define how the 3D scene is transformed into a 2D image on the camera sensor. They are all measured in pixels and are based on an approximation of the camera as a pinhole camera. They can be roughly calculated using the camera and lens specs.\n",
    "\n",
    "They are:\n",
    "- Focal distance $f_x$, $f_y$: distance between the \"pinhole\" and the image. These are assumed to be equal.\n",
    "- Principle point offset $x_0$, $y_0$: 2D distance between the image center and the pinhole center.\n",
    "- Axis skew $s$: a measure of distortion, which we will estimate as 0.\n",
    "\n",
    "\n",
    "One useful framing of these parameters is in terms of a matrix decomposition that breaks down the contribution of each set of parameters to the final image. Consider a matrix of intrinsics $K$.\n",
    "\n",
    "$$\n",
    "K = \\begin{bmatrix}f_x & s & x_0 \\\\ 0 & f_y & y_0\\\\ 0 & 0 & 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "This matrix decomposes into a *2D translation* defined by $x_0$ and $y_0$, a *2D scaling* defined by $f_x$ and $f_y$, and a *2D shear* defined by $s$:\n",
    "\n",
    "$$\n",
    "K = \\begin{bmatrix}1 & 0 & x_0 \\\\ 0 & 1 & y_0\\\\ 0 & 0 & 1 \\end{bmatrix} * \n",
    "\\begin{bmatrix}f_x & 0 & 0 \\\\ 0 & f_y & 0\\\\ 0 & 0 & 1 \\end{bmatrix} * \n",
    "\\begin{bmatrix}1 & \\frac{s}{f_x} & 0 \\\\ 0 & 1 & 0\\\\ 0 & 0 & 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "To calculate these things, you need to know what type of camera and lens you're using. I'm using a Flir [Blackfly BFS-U3-88S6C-C](https://www.edmundoptics.com/p/bfs-u3-88s6c-c-usb3-blackflyreg-s-color-camera/40168/), which has a [Sony IMX267](https://www.1stvision.com/cameras/sensor_specs/IMX267-sensor-spec.pdf) sensor, and a 12.5mm lens.\n",
    "\n",
    "### Focal distance\n",
    "This is the distance between the pinhole and the image, which in our case is basically the focal length of the lens. This should be the same for all the cameras unless you're using a different lens for any of them. To get the focal distance in pixels, we can use the focal length of the lens and the sensor dimensions. Let $W$ be the sensor width in mm, $w$ be the width in pixels, and $F_x$ be the focal length in mm. We can calculate focal distance in pixels, $f_x$ with:\n",
    "\n",
    "$$f_x = F_x \\frac{w}{W}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Define the lens and sensor params '''\n",
    "# focal length of your lens\n",
    "Fx = 12.5 # mm\n",
    "\n",
    "# these are from the sensor datasheet and cropping params in Spinnaker\n",
    "pixel_size = 6.9e-3 # mm - binned pixel size\n",
    "w_pixels = 1896 # width of the cropped image in pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "focal distance is 1811.59 pixels\n"
     ]
    }
   ],
   "source": [
    "''' Calculate focal distance in pixels '''\n",
    "# use pixel size and image width in pixels to determine W\n",
    "W_mm = pixel_size * w_pixels # width of the cropped image in mm\n",
    "\n",
    "# compute focal distance (pixels)\n",
    "fx = Fx * (w_pixels / W_mm)\n",
    "\n",
    "print(f'focal distance is {np.round(fx, 2)} pixels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle point offset\n",
    "This is just the pixel distance from the origin (top left corner in Blackfly cameras) to the image center. To compute this, simply divide your image height/width by 2 and add the x and y offset (all found in Spinnaker, image format). Note that this may be slightly different for each camera, depending on how they were positioned and cropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Input your image params '''\n",
    "image_w = 1896 # pixels\n",
    "image_h = 640 # pixels\n",
    "x_shift = 80\n",
    "y_shift = 40\n",
    "\n",
    "# my blue camera is positioned a little differently from the others..\n",
    "x_shift_blue = 48\n",
    "y_shift_blue = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Calculate offsets '''\n",
    "x0 = (image_w / 2) + x_shift\n",
    "x0_blue = (image_w / 2) + x_shift_blue\n",
    "y0 = (image_h / 2) + y_shift\n",
    "y0_blue = (image_h / 2) + y_shift_blue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save everything in a camera array\n",
    "Finally, we can stick all these params into an array for each camera, which we'll use to initialize the camera array in [pySBA](https://github.com/jahdiel/pySBA/blob/master/PySBA.py).\n",
    "\n",
    "\n",
    "Camera parameters are:\n",
    "\n",
    "\n",
    "**Extrinsics**\n",
    "- A 3D rotation vector that rotates the world coordinate axes into camera coordinate axes; array of floats, shape (3, )\n",
    "- A 3D translation vector that translates the world origin to the camera origin; array of floats, shape (3, )\n",
    "\n",
    "\n",
    "**Intrinsics**\n",
    "- Focal distance in pixels; float\n",
    "- Distortion params; array of floats, shape (2, ) (we'll call these 0 for now)\n",
    "- Principal point offsets (x, y); array of ints, shape (2, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_ids = ['red_cam', 'yellow_cam', 'green_cam', 'blue_cam']\n",
    "file_path = 'C:/Users/ilow1/Documents/Python Scripts/bird_pose_tracking/calibration_files/'\n",
    "folder = 'init_cam_arrays/'\n",
    "\n",
    "intrinsics = np.asarray([fx, 0, 0, x0, y0])\n",
    "intrinsics_blue = np.asarray([fx, 0, 0, x0_blue, y0_blue])\n",
    "\n",
    "for i, cam in enumerate(cam_ids):\n",
    "    if cam == 'blue_cam':\n",
    "        cam_array = np.append(rotation_vecs[i], np.append(t, intrinsics_blue))\n",
    "    else:\n",
    "        cam_array = np.append(rotation_vecs[i], np.append(t, intrinsics))\n",
    "    np.save(f'{file_path}{folder}{cam}_array.npy', cam_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
